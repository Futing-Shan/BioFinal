{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76386598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb15059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集size：[5148, 256, 256] label size: [5148]\n",
    "# 测试集size：[1292, 256, 256] label size: [1292]\n",
    "# label covid: 0, normal: 1, pneumonia: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982e4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像预处理\n",
    "# 将图像修改为统一的尺寸（256 * 256）\n",
    "# 将图像转为灰度图像（值为0-255的二维矩阵）\n",
    "train_input = []\n",
    "train_label = []\n",
    "train_img_dir = './Data/train/'\n",
    "\n",
    "for i in range(460):\n",
    "    train_covid_img_path = train_img_dir + 'COVID19/COVID19(' + str(i) +  ').jpg'\n",
    "    train_covid_img = Image.open(train_covid_img_path)\n",
    "    train_covid_img_resize = train_covid_img.resize((256, 256))\n",
    "    train_covid_img_resize = train_covid_img_resize.convert(\"L\")\n",
    "    train_input.append(np.asarray(train_covid_img_resize) / 255)\n",
    "    train_label.append(0)\n",
    "\n",
    "for i in range(1266):\n",
    "    train_normal_img_path = train_img_dir + 'NORMAL/NORMAL(' + str(i) + ').jpg'\n",
    "    train_normal_img = Image.open(train_normal_img_path)\n",
    "    train_normal_img_resize = train_normal_img.resize((256, 256))\n",
    "    train_normal_img_resize = train_normal_img_resize.convert(\"L\")\n",
    "    train_input.append(np.asarray(train_normal_img_resize) / 255)\n",
    "    train_label.append(1)\n",
    "    \n",
    "for i in range(3418):\n",
    "    train_pneumonia_img_path = train_img_dir + 'PNEUMONIA/PNEUMONIA(' + str(i) + ').jpg'\n",
    "    train_pneumonia_img = Image.open(train_pneumonia_img_path)\n",
    "    train_pneumonia_img_resize = train_pneumonia_img.resize((256, 256))\n",
    "    train_pneumonia_img_resize = train_pneumonia_img_resize.convert(\"L\")\n",
    "    train_input.append(np.asarray(train_pneumonia_img_resize) / 255)\n",
    "    train_label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c051615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5144, 256, 256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = np.array(train_input)\n",
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac51f139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5144,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = np.array(train_label)\n",
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c24cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = []\n",
    "test_label = []\n",
    "test_img_dir = './Data/test/'\n",
    "\n",
    "for i in range(460, 576):\n",
    "    test_covid_img_path = test_img_dir + 'COVID19/COVID19(' + str(i) +  ').jpg'\n",
    "    test_covid_img = Image.open(test_covid_img_path)\n",
    "    test_covid_img_resize = test_covid_img.resize((256, 256))\n",
    "    test_covid_img_resize = test_covid_img_resize.convert(\"L\")\n",
    "    test_input.append(np.asarray(test_covid_img_resize) / 255)\n",
    "    test_label.append(0)\n",
    "    \n",
    "for i in range(1266, 1583):\n",
    "    test_normal_img_path = test_img_dir + 'NORMAL/NORMAL(' + str(i) + ').jpg'\n",
    "    test_normal_img = Image.open(test_normal_img_path)\n",
    "    test_normal_img_resize = test_normal_img.resize((256, 256))\n",
    "    test_normal_img_resize = test_normal_img_resize.convert(\"L\")\n",
    "    test_input.append(np.asarray(test_normal_img_resize) / 255)\n",
    "    test_label.append(1)\n",
    "    \n",
    "for i in range(3418, 4273):\n",
    "    test_pneumonia_img_path = test_img_dir + 'PNEUMONIA/PNEUMONIA(' + str(i) + ').jpg'\n",
    "    test_pneumonia_img = Image.open(test_pneumonia_img_path)\n",
    "    test_pneumonia_img_resize = test_pneumonia_img.resize((256, 256))\n",
    "    test_pneumonia_img_resize = test_pneumonia_img_resize.convert(\"L\")\n",
    "    test_input.append(np.asarray(test_pneumonia_img_resize) / 255)\n",
    "    test_label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185edeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1288, 256, 256)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = np.array(test_input)\n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f6df57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1288,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label = np.array(test_label)\n",
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66d2e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=65536, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (fc3): Linear(in_features=60, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 搭建CNN神经网络\n",
    "# torch.nn.Conv2d\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        # 1 input image channel ,6 output channels,3x3 square convolution kernel\n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=6,kernel_size=3,padding=1)\n",
    "        \n",
    "        # 6 input channl,16 output channels,3x3 square convolution kernel\n",
    "        self.conv2=nn.Conv2d(in_channels=6,out_channels=16,kernel_size=3, padding=1)\n",
    "        \n",
    "        # an affine operation:y=Wx+b\n",
    "        self.fc1=nn.Linear(16*64*64,120)\n",
    "        self.fc2=nn.Linear(120,60)\n",
    "        self.fc3=nn.Linear(60,3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x是网络的输入，然后将x前向传播，最后得到输出\n",
    "        # 下面两句定义了两个2x2的池化层\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        # if the size is square you can only specify a single number\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x=x.view(-1,self.num_flat_features(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size=x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features=1\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "cnn=CNN().double()\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18217518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\Users\\builder\\tkoch\\workspace\\pytorch\\pytorch_1647970138273\\work\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 24272437248 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m cnn_train_labelv \u001b[38;5;241m=\u001b[39m cnn_train_labelv\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m output_train \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_train_inputv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output_train, cnn_train_labelv)\n\u001b[0;32m     28\u001b[0m train_loss_arr\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\.conda\\envs\\Bio\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [9], line 23\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# x是网络的输入，然后将x前向传播，最后得到输出\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# 下面两句定义了两个2x2的池化层\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mmax_pool2d(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m),(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# if the size is square you can only specify a single number\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mmax_pool2d(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)),\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Bio\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\Bio\\lib\\site-packages\\torch\\nn\\modules\\conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\Bio\\lib\\site-packages\\torch\\nn\\modules\\conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\Users\\builder\\tkoch\\workspace\\pytorch\\pytorch_1647970138273\\work\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 24272437248 bytes."
     ]
    }
   ],
   "source": [
    "cnn_train_input = train_input.reshape(train_input.shape[0], 1, train_input.shape[1], train_input.shape[2])\n",
    "cnn_train_input = np.float64(cnn_train_input)\n",
    "cnn_train_input  = torch.from_numpy(cnn_train_input)\n",
    "\n",
    "cnn_train_label = train_label.astype(int);\n",
    "cnn_train_label = torch.from_numpy(cnn_train_label)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "cnn_training_epochs = 40\n",
    "\n",
    "train_loss_arr = []\n",
    "# val_loss_arr = []\n",
    "for epoch in range(cnn_training_epochs):\n",
    "    print(1)\n",
    "    cnn.train()\n",
    "    \n",
    "    cnn_train_inputv = Variable(cnn_train_input)\n",
    "    cnn_train_labelv = Variable(cnn_train_label)\n",
    "    cnn_train_labelv = cnn_train_labelv.long()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output_train = cnn(cnn_train_inputv)\n",
    "    \n",
    "    loss = criterion(output_train, cnn_train_labelv)\n",
    "    train_loss_arr.append(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "#     cnn_val_inputv = Variable(cnn_val_input)\n",
    "#     cnn_val_labelv = Variable(cnn_val_label)\n",
    "#     cnn_val_labelv = cnn_val_labelv.long()\n",
    "    \n",
    "#     output_val = cnn_model(cnn_val_inputv)\n",
    "    \n",
    "#     loss_val = criterion(output_val, cnn_val_labelv)\n",
    "#     val_loss_arr.append(loss_val.item())\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == cnn_training_epochs - 1:\n",
    "        print(\"[%d/%d] Training Loss: %.4f\" % (epoch, cnn_training_epochs, loss.item()))\n",
    "#         print(\"[%d/%d] Validation Loss: %.4f\" % (epoch, cnn_training_epochs, loss_val.item()))\n",
    "#  <caution:Need more memory> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deec64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce45abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试CNN模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bc1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73deae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16f8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bio",
   "language": "python",
   "name": "bio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
